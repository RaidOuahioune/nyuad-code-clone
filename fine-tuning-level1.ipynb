{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd3e8578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T23:49:51.282948Z",
     "iopub.status.busy": "2024-08-07T23:49:51.282512Z",
     "iopub.status.idle": "2024-08-07T23:49:55.806133Z",
     "shell.execute_reply": "2024-08-07T23:49:55.805352Z"
    },
    "papermill": {
     "duration": 4.534295,
     "end_time": "2024-08-07T23:49:55.808394",
     "exception": false,
     "start_time": "2024-08-07T23:49:51.274099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d9f7782",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T23:49:55.821026Z",
     "iopub.status.busy": "2024-08-07T23:49:55.820626Z",
     "iopub.status.idle": "2024-08-07T23:49:55.824930Z",
     "shell.execute_reply": "2024-08-07T23:49:55.823736Z"
    },
    "papermill": {
     "duration": 0.012503,
     "end_time": "2024-08-07T23:49:55.826896",
     "exception": false,
     "start_time": "2024-08-07T23:49:55.814393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install torchtriton --extra-index-url \"https://download.pytorch.org/whl/nightly/cu117\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aac3faa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T23:49:55.838754Z",
     "iopub.status.busy": "2024-08-07T23:49:55.838491Z",
     "iopub.status.idle": "2024-08-07T23:49:55.933291Z",
     "shell.execute_reply": "2024-08-07T23:49:55.932301Z"
    },
    "papermill": {
     "duration": 0.10302,
     "end_time": "2024-08-07T23:49:55.935520",
     "exception": false,
     "start_time": "2024-08-07T23:49:55.832500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to cuda.\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set the device to GPU if available, otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device set to {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b149d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T23:49:55.947547Z",
     "iopub.status.busy": "2024-08-07T23:49:55.947270Z",
     "iopub.status.idle": "2024-08-07T23:49:55.951408Z",
     "shell.execute_reply": "2024-08-07T23:49:55.950361Z"
    },
    "papermill": {
     "duration": 0.012242,
     "end_time": "2024-08-07T23:49:55.953334",
     "exception": false,
     "start_time": "2024-08-07T23:49:55.941092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR=\"./../input/fine-tuning-data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "488706a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T23:49:55.965299Z",
     "iopub.status.busy": "2024-08-07T23:49:55.964991Z",
     "iopub.status.idle": "2024-08-07T23:49:55.984093Z",
     "shell.execute_reply": "2024-08-07T23:49:55.983283Z"
    },
    "papermill": {
     "duration": 0.02724,
     "end_time": "2024-08-07T23:49:55.985998",
     "exception": false,
     "start_time": "2024-08-07T23:49:55.958758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 36 (inside ./../input/fine-tuning-data/meta.pkl)\n"
     ]
    }
   ],
   "source": [
    "# Attempt to derive vocab_size from the dataset\n",
    "\n",
    "meta_path = os.path.join(DATA_DIR, 'meta.pkl')\n",
    "vocab_size = None\n",
    "\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {vocab_size} (inside {meta_path})\")\n",
    "else:\n",
    "    print(\"Meta file not found. Please ensure the meta.pkl file is present in the data directory.\")\n",
    "\n",
    "# Encode and decode functions for character-level Tokenzation \n",
    "def encode(s):\n",
    "    return [meta['stoi'][c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    result=[]\n",
    "    for i in l:\n",
    "        try:\n",
    "            result.append(meta['itos'][i])\n",
    "        except:\n",
    "             result.append(\"-\")\n",
    "    return \"\".join(result)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a18ac14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T23:49:55.997849Z",
     "iopub.status.busy": "2024-08-07T23:49:55.997579Z",
     "iopub.status.idle": "2024-08-07T23:49:56.007697Z",
     "shell.execute_reply": "2024-08-07T23:49:56.007028Z"
    },
    "papermill": {
     "duration": 0.018156,
     "end_time": "2024-08-07T23:49:56.009694",
     "exception": false,
     "start_time": "2024-08-07T23:49:55.991538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load encoded data\n",
    "train_data = np.memmap(os.path.join(DATA_DIR, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(DATA_DIR, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "model_path = \"/kaggle/input/tiny-llm-10m/pytorch/default/1/10M_2024-07-21_08-16.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da1ee38a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T23:49:56.021367Z",
     "iopub.status.busy": "2024-08-07T23:49:56.021081Z",
     "iopub.status.idle": "2024-08-07T23:49:56.026779Z",
     "shell.execute_reply": "2024-08-07T23:49:56.026008Z"
    },
    "papermill": {
     "duration": 0.013595,
     "end_time": "2024-08-07T23:49:56.028629",
     "exception": false,
     "start_time": "2024-08-07T23:49:56.015034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters for the GPT model\n",
    "block_size = 256  # Maximum context length\n",
    "vocab_size=53\n",
    "n_embd = 372      # Embedding dimension\n",
    "n_head = 6        # Number of attention heads\n",
    "n_layer = 6       # Number of transformer blocks\n",
    "dropout = 0       # Dropout rate\n",
    "batch_size = 64   # Batch size for training\n",
    "max_iters = 60000  # Maximum number of iterations\n",
    "learning_rate = 1e-3 # Initial Learning rate value\n",
    "miles = [int(max_iters * m) for m in [0.7, 0.8, 0.9]]  # Milestones for learning rate decay as fractions of max_iters\n",
    "eval_interval = 10000 # Evaluation interval\n",
    "eval_iters = 1000  # Number of iterations for evaluation\n",
    "\n",
    "compile = False # requires PyTorch 2.0\n",
    "\n",
    "# Rank value for lora\n",
    "\n",
    "r=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6aa7edc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T23:49:56.040671Z",
     "iopub.status.busy": "2024-08-07T23:49:56.040432Z",
     "iopub.status.idle": "2024-08-07T23:49:56.065765Z",
     "shell.execute_reply": "2024-08-07T23:49:56.064960Z"
    },
    "papermill": {
     "duration": 0.033673,
     "end_time": "2024-08-07T23:49:56.067734",
     "exception": false,
     "start_time": "2024-08-07T23:49:56.034061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B, T, 16)\n",
    "        q = self.query(x) # (B, T, 16)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=dropout if self.training else 0, is_causal=True)\n",
    "            \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self-attention in parallel.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear( 4 * n_embd, n_embd, bias=False),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by feedforward.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd, bias=False)\n",
    "        self.ln2 = nn.LayerNorm(n_embd, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd, bias=False) \n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:] # (B, T)\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7f051fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T23:49:56.079892Z",
     "iopub.status.busy": "2024-08-07T23:49:56.079657Z",
     "iopub.status.idle": "2024-08-07T23:49:56.097467Z",
     "shell.execute_reply": "2024-08-07T23:49:56.096676Z"
    },
    "papermill": {
     "duration": 0.025824,
     "end_time": "2024-08-07T23:49:56.099222",
     "exception": false,
     "start_time": "2024-08-07T23:49:56.073398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LoraHead(Head):\n",
    "    \"\"\"\n",
    "    Extends MultiHeadAttention with LoRA (Low-Rank Adaptation) matrices.\n",
    "    LoRA enhances efficiency by only updating the query and value matrices.\n",
    "    This class adds LoRA matrices and applies LoRA logic in the forward method.\n",
    "\n",
    "    Parameters:\n",
    "    - r (int): Rank for LoRA matrices.\n",
    "    - config: Configuration of the Roberta Model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, r=8):\n",
    "        head_size = n_embd // n_head\n",
    "        super().__init__(head_size=head_size)\n",
    "        \n",
    "        self.lora_query_matrix_A = nn.Parameter(torch.zeros(r, head_size))\n",
    "        self.lora_query_matrix_B = nn.Parameter(torch.randn(n_embd, r))\n",
    "        self.lora_value_matrix_A = nn.Parameter(torch.zeros(r, head_size))\n",
    "        self.lora_value_matrix_B = nn.Parameter(torch.randn(n_embd, r))\n",
    "        self.lora_key_matrix_A = nn.Parameter(torch.zeros(r, head_size))\n",
    "        self.lora_key_matrix_B = nn.Parameter(torch.randn(n_embd, r))\n",
    "\n",
    "    def lora_query(self, x):\n",
    "        \"\"\"\n",
    "        Applies LoRA to the query component. Computes a modified query output by adding \n",
    "        the LoRA adaptation to the standard query output. Requires the regular linear layer \n",
    "        to be frozen before training.\n",
    "        \"\"\"\n",
    "        lora_query_weights = torch.matmul(self.lora_query_matrix_B, self.lora_query_matrix_A)\n",
    "\n",
    "        return self.query(x) + F.linear(x, lora_query_weights.T)\n",
    "\n",
    "    def lora_value(self, x):\n",
    "        \"\"\"\n",
    "        Applies LoRA to the value component. Computes a modified value output by adding \n",
    "        the LoRA adaptation to the standard value output. Requires the regular linear layer \n",
    "        to be frozen before training.\n",
    "        \"\"\"\n",
    "        lora_value_weights = torch.matmul(self.lora_value_matrix_B, self.lora_value_matrix_A)\n",
    "\n",
    "\n",
    "        return self.value(x) +  F.linear(x, lora_value_weights.T)\n",
    "\n",
    "    def lora_key(self, x):\n",
    "        \"\"\"\n",
    "        Applies LoRA to the key component. Computes a modified value output by adding \n",
    "        the LoRA adaptation to the standard value output. Requires the regular linear layer \n",
    "        to be frozen before training.\n",
    "        \"\"\"\n",
    "        lora_key_weights = torch.matmul(self.lora_key_matrix_B, self.lora_key_matrix_A)\n",
    "\n",
    "        return self.key(x) +  F.linear(x, lora_key_weights.T)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.lora_key(x)\n",
    "        q = self.lora_query(x)\n",
    "        v = self.lora_value(x)\n",
    "\n",
    "        out = torch.nn.functional.scaled_dot_product_attention(\n",
    "            q, k, v, attn_mask=None, dropout_p=dropout if self.training else 0, is_causal=True\n",
    "        )\n",
    "\n",
    "        return out\n",
    "class LoraGPT(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __load_model(self)->GPT:\n",
    "        \n",
    "\n",
    "\n",
    "        model = GPT()\n",
    "        #try:\n",
    "        #    model = torch.compile(model)  # requires PyTorch 2.0\n",
    "        #except Exception as e:\n",
    "        #    pass\n",
    "        model.load_state_dict(torch.load(model_path),strict=False)\n",
    "\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def __init__(self,  r=8):\n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        self.lora_rank = r\n",
    "  \n",
    "        self.model=self.__load_model()\n",
    "        self.replace_multihead_attention_recursion(self.model)\n",
    "        self.freeze_parameters_except_lora_and_bias()\n",
    "        \n",
    "        \n",
    "    def forward(self, x,targets):\n",
    "        return self.model(x,targets)\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        return self.model.generate(idx, max_new_tokens)\n",
    "   \n",
    "        \n",
    "    def replace_multihead_attention_recursion(self,model):\n",
    "        \"\"\"\n",
    "        Replaces RobertaSelfAttention with LoraRobertaSelfAttention in the model.\n",
    "        This method applies the replacement recursively to all sub-components.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : nn.Module\n",
    "            The PyTorch module or model to be modified.\n",
    "        \"\"\"\n",
    "        for name, module in model.named_children():\n",
    "            if isinstance(module, Head):\n",
    "                # Replace RobertaSelfAttention with LoraRobertaSelfAttention\n",
    "                new_layer = LoraHead(r=self.lora_rank)\n",
    "                new_layer.load_state_dict(module.state_dict(), strict=False)\n",
    "                setattr(model, name, new_layer)\n",
    "            else:\n",
    "                # Recursive call for child modules\n",
    "                self.replace_multihead_attention_recursion(module)\n",
    "                \n",
    "                \n",
    "    def freeze_parameters_except_lora_and_bias(self):\n",
    "        \"\"\"\n",
    "        Freezes all model parameters except for specific layers and types based on the configuration.\n",
    "        Parameters in LoRA layers, the finetune head, bias parameters, embeddings, and layer norms \n",
    "        can be set as trainable based on class settings.\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "\n",
    "            is_trainable = (\n",
    "                \"lora_\" in name \n",
    "                \n",
    "                #(self.train_layer_norms and \"LayerNorm\" in name)\n",
    "            )\n",
    "            param.requires_grad = is_trainable\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af255332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T23:49:56.110940Z",
     "iopub.status.busy": "2024-08-07T23:49:56.110691Z",
     "iopub.status.idle": "2024-08-07T23:49:57.197167Z",
     "shell.execute_reply": "2024-08-07T23:49:57.196394Z"
    },
    "papermill": {
     "duration": 1.094787,
     "end_time": "2024-08-07T23:49:57.199410",
     "exception": false,
     "start_time": "2024-08-07T23:49:56.104623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#model=GPT().to(device)\n",
    "model = LoraGPT(r=r)\n",
    "m=model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6262f97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T23:49:57.211819Z",
     "iopub.status.busy": "2024-08-07T23:49:57.211337Z",
     "iopub.status.idle": "2024-08-07T23:49:57.217428Z",
     "shell.execute_reply": "2024-08-07T23:49:57.216627Z"
    },
    "papermill": {
     "duration": 0.014377,
     "end_time": "2024-08-07T23:49:57.219438",
     "exception": false,
     "start_time": "2024-08-07T23:49:57.205061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374976\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bff6798b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T23:49:57.231257Z",
     "iopub.status.busy": "2024-08-07T23:49:57.230991Z",
     "iopub.status.idle": "2024-08-07T23:49:57.239081Z",
     "shell.execute_reply": "2024-08-07T23:49:57.238262Z"
    },
    "papermill": {
     "duration": 0.016041,
     "end_time": "2024-08-07T23:49:57.240876",
     "exception": false,
     "start_time": "2024-08-07T23:49:57.224835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get random batch of data\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Estimate loss on train and val splits\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters) \n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b773ae8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T23:49:57.252627Z",
     "iopub.status.busy": "2024-08-07T23:49:57.252167Z",
     "iopub.status.idle": "2024-08-07T23:49:58.583707Z",
     "shell.execute_reply": "2024-08-07T23:49:58.582729Z"
    },
    "papermill": {
     "duration": 1.340017,
     "end_time": "2024-08-07T23:49:58.586216",
     "exception": false,
     "start_time": "2024-08-07T23:49:57.246199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=miles, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77b17f73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T23:49:58.601921Z",
     "iopub.status.busy": "2024-08-07T23:49:58.601387Z",
     "iopub.status.idle": "2024-08-08T08:44:06.713698Z",
     "shell.execute_reply": "2024-08-08T08:44:06.712693Z"
    },
    "papermill": {
     "duration": 32048.159023,
     "end_time": "2024-08-08T08:44:06.752410",
     "exception": false,
     "start_time": "2024-08-07T23:49:58.593387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss estimation\n",
      "iter     0 | train loss 4.1366 | val loss 4.1372\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "loss estimation\n",
      "iter 10000 | train loss 0.4729 | val loss 0.4714\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "loss estimation\n",
      "iter 20000 | train loss 0.4452 | val loss 0.4439\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "loss estimation\n",
      "iter 30000 | train loss 0.4561 | val loss 0.4546\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "loss estimation\n",
      "iter 40000 | train loss 0.4291 | val loss 0.4282\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "loss estimation\n",
      "iter 50000 | train loss 0.4234 | val loss 0.4236\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "passed 50 iteration\n",
      "Training time: 534.130280649662  min\n"
     ]
    }
   ],
   "source": [
    "# Get current date and hour to get track of experiments\n",
    "now = datetime.datetime.now()\n",
    "date_hour = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "# Train\n",
    "# Start training timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # evaluate the model on the train and val splits and log the losses\n",
    "    if iter % eval_interval == 0:\n",
    "        print(\"loss estimation\")\n",
    "        losses = estimate_loss()\n",
    "        print(f'iter {iter:5d} | train loss {losses[\"train\"]:.4f} | val loss {losses[\"val\"]:.4f}')\n",
    "        \n",
    "    # train the model for one iteration\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Step the scheduler\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    scheduler.step()\n",
    "    if iter%50==0:\n",
    "        print('passed 50 iteration')\n",
    "\n",
    "# End training timer\n",
    "end_time = time.time()\n",
    "print(f'Training time: {(end_time - start_time) / 60}  min')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), f\"{num_parameters}_{date_hour}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c349d2ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T08:44:06.948919Z",
     "iopub.status.busy": "2024-08-08T08:44:06.948587Z",
     "iopub.status.idle": "2024-08-08T08:44:06.972926Z",
     "shell.execute_reply": "2024-08-08T08:44:06.972260Z"
    },
    "papermill": {
     "duration": 0.125065,
     "end_time": "2024-08-08T08:44:06.974797",
     "exception": false,
     "start_time": "2024-08-08T08:44:06.849732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = np.memmap(os.path.join(DATA_DIR, 'test.bin'), dtype=np.uint16, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d401531",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T08:44:07.168445Z",
     "iopub.status.busy": "2024-08-08T08:44:07.168153Z",
     "iopub.status.idle": "2024-08-08T08:45:00.095776Z",
     "shell.execute_reply": "2024-08-08T08:45:00.094814Z"
    },
    "papermill": {
     "duration": 53.027491,
     "end_time": "2024-08-08T08:45:00.098006",
     "exception": false,
     "start_time": "2024-08-08T08:44:07.070515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:51<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def find_first_print(text):\n",
    "    pattern = r'\\nprint\\('\n",
    "    match = re.search(pattern, text)\n",
    "    \n",
    "    if not match:\n",
    "        return None\n",
    "    \n",
    "    start = match.start()  # Start from the newline character\n",
    "    stack = 1\n",
    "    i = start + len(\"\\nprint(\")  # Move index to the character after 'print('\n",
    "    \n",
    "    while i < len(text) and stack > 0:\n",
    "        if text[i] == '(':\n",
    "            stack += 1\n",
    "        elif text[i] == ')':\n",
    "            stack -= 1\n",
    "        i += 1\n",
    "    \n",
    "    if stack == 0:\n",
    "        return text[start:i]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def evaluate_example(example, model, max_new_tokens=30):\n",
    "\n",
    "    # Split example and determine maximum new tokens allowed\n",
    "    splited_example = example.split(\"# reformulation\")\n",
    "    if not (\"for\" in splited_example[0]):\n",
    "        max_new_tokens = 22\n",
    "    # Encode prompt and prepare for evaluation\n",
    "    encoded_example = torch.tensor(encode(splited_example[0] + \"# reformulation\"), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    prompt_text = splited_example[0] + \"# reformulation\"\n",
    "\n",
    "    result_example = splited_example[-1]\n",
    "\n",
    "    #print(\"result: ==>\",result_example)\n",
    "\n",
    "    # Extract real results from example\n",
    "    #real_results = [float(match.group()) for match in re.finditer(r\"(?<=# )-?\\d+(\\.\\d+)?\", result_example.split('\\n\\n')[0].replace(\"\\n\", \"\"))]\n",
    "\n",
    "    # Generate response from model and extract generated results\n",
    "    response = decode(model.generate(encoded_example, max_new_tokens=max_new_tokens)[0].tolist())\n",
    "    splited_response = response.split(\"# reformulation\")\n",
    "    result_response = splited_response[-1]\n",
    "    \n",
    "\n",
    "    result_response=find_first_print(result_response)\n",
    "    if result_response is None:\n",
    "        result_response = splited_response[-1]\n",
    "    \n",
    "    #generated_results = [float(match.group()) for match in re.finditer(r\"(?<=# )-?\\d+(\\.\\d+)?\", result_response.split('\\n\\n')[0].replace(\"\\n\", \"\"))]\n",
    "\n",
    "    return prompt_text, result_example, result_response\n",
    "\n",
    "\n",
    "\n",
    "# Write results to file\n",
    "def write_results_to_file(output_file, prompt, real_results, generated_results):\n",
    "    df = pd.DataFrame({\n",
    "        'Prompt': prompt,\n",
    "        'Real_Results': real_results,\n",
    "        'Generated_Results': generated_results\n",
    "    })\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_pair(real, generated_result):\n",
    "    # Determine the length of the shorter and longer strings\n",
    "    min_len = min(len(real), len(generated_result))\n",
    "    max_len = max(len(real), len(generated_result))\n",
    "\n",
    "    # Count the number of matching characters at the same index\n",
    "    match_count = sum(1 for i in range(min_len) if real[i] == generated_result[i])\n",
    "\n",
    "    # Calculate the ratio of matches to the length of the longer string\n",
    "    ratio = match_count / max_len\n",
    "    return ratio\n",
    "\n",
    "# Evaluation Loop\n",
    "\n",
    "# Split examples and initialize lists for results\n",
    "examples = decode(test_data).split(\"\\n\\n\")\n",
    "examples = [example for example in examples if example]\n",
    "\n",
    "# Start evaluation process\n",
    "prompt = []\n",
    "real_results = []\n",
    "generated_results = []\n",
    "\n",
    "# Iterate through examples and evaluate the model on each one\n",
    "for example in tqdm(examples[:100]):\n",
    "    try:\n",
    "        prompt_text, real_result, result = evaluate_example(example, model)\n",
    "        prompt.append(prompt_text)\n",
    "        real_results.append(real_result)\n",
    "        generated_results.append(result)\n",
    "    except:\n",
    "        prompt.append(prompt_text)\n",
    "        real_results.append(real_result)\n",
    "        generated_results.append(result)\n",
    "        \n",
    "    \n",
    "\n",
    "# Calculate and print accuracy\n",
    "score=0\n",
    "\n",
    "for real,generated in zip(real_results, generated_results):\n",
    "  score+=evaluate_pair(real,generated)\n",
    "accuracy = score / len(generated_results)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Store accuracy in a file\n",
    "with open(\"accuracy.txt\", 'w') as f:\n",
    "    f.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "# Store predictions in a CSV file\n",
    "    write_results_to_file(\"predictions.csv\", prompt, real_results, generated_results)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5498826,
     "sourceId": 9110656,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 99718,
     "modelInstanceId": 74994,
     "sourceId": 89395,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32113.437228,
   "end_time": "2024-08-08T08:45:01.792553",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-07T23:49:48.355325",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
