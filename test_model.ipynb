{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256  # Maximum context length\n",
    "n_embd = 372      # Embedding dimension\n",
    "n_head = 6        # Number of attention heads\n",
    "n_layer = 6       # Number of transformer blocks\n",
    "dropout = 0       # Dropout rate\n",
    "batch_size = 64   # Batch size for training\n",
    "max_iters = 20000  # Maximum number of iterations\n",
    "learning_rate = 1e-3 # Initial Learning rate value\n",
    "miles = [int(max_iters * m) for m in [0.7, 0.8, 0.9]]  # Milestones for learning rate decay as fractions of max_iters\n",
    "eval_interval = 29_000 # Evaluation interval\n",
    "eval_iters = 60000 # Number of iterations for evaluation\n",
    "vocab_size = 53 # Vocabulary size\n",
    "\n",
    "# Model to be fine-tuned (Keep it empty for training from scratch)\n",
    "model_name = '1M_2024-08-21_23-47'\n",
    "model_path = \"1M_2024-08-21_23-47.pth\"\n",
    "\n",
    "#Lora params\n",
    "heads_only=False # fine tune only on attention head or all linear layers\n",
    "# LoRA Rank - Set it to 0 if you want to train from scratch or perform full fine-tuning\n",
    "lora_r = 20\n",
    "# Lora Alpha\n",
    "lora_alpha=8\n",
    "lora_dropout=0\n",
    "\n",
    "compile = False\n",
    "# enable quntization for the fine tuning (8 bit quantization)\n",
    "quantize=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm with an optional bias. PyTorch's LayerNorm doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        out = torch.nn.functional.scaled_dot_product_attention(\n",
    "            q, k, v, attn_mask=None, dropout_p=dropout if self.training else 0, is_causal=True\n",
    "        )\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Concatenate the outputs from each head\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple linear layer followed by a non-linearity.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd, bias=False),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class LinearLoRA(nn.Module):\n",
    "    def __init__(self, original_layer, rank=8):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.original_layer.weight.requires_grad = False\n",
    "        self.rank = rank\n",
    "        \n",
    "        self.lora_a = nn.Parameter(torch.randn((original_layer.in_features, rank)))\n",
    "        self.lora_b = nn.Parameter(torch.randn((rank, original_layer.out_features)))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.lora_a, a=np.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_b)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lora_output = x @ self.lora_a @ self.lora_b\n",
    "        return self.original_layer(x) + lora_output\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by feedforward.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd, bias=False)\n",
    "        self.ln2 = nn.LayerNorm(n_embd, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"GPT language model.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd, bias=False) \n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Token and position embeddings\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device='cpu')) # (T, n_embd)\n",
    "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
    "        x = self.blocks(x) # (B, T, n_embd)\n",
    "        x = self.ln_f(x) # (B, T, n_embd)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        # Compute loss if targets are provided\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"Generate new tokens given an initial context `idx`.\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:] # Crop to the last block_size tokens\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # Focus on the last time step\n",
    "            probs = F.softmax(logits, dim=-1) # Convert to probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # Sample from the distribution\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # Append sampled index to the sequence\n",
    "        return idx\n",
    "    \n",
    "    def activate_lora(self, r=8, heads_only=False, freeze_others=True):\n",
    "        self.lora_rank = r\n",
    "        self.replace_multihead_attention_recursion(heads_only)\n",
    "        if freeze_others:\n",
    "            self.freeze_parameters_except_lora_and_bias()\n",
    "    \n",
    "    def replace_multihead_attention_recursion(self, heads_only=False, model=None):\n",
    "        children = self.named_children() if model is None else model.named_children()\n",
    "        for name, module in children:\n",
    "            if heads_only and name in {\"query\", \"key\", \"value\"}:\n",
    "                # Replace with Lora SelfAttention\n",
    "                new_layer = LinearLoRA(module, rank=self.lora_rank)\n",
    "\n",
    "                if model == None:\n",
    "                    self.__setattr__(name, new_layer)\n",
    "                else:\n",
    "                    setattr(model, name, new_layer)\n",
    "            \n",
    "            elif isinstance(module, nn.Linear) and not heads_only:\n",
    "                new_layer = LinearLoRA(module, rank=self.lora_rank)\n",
    "                \n",
    "                if model == None:\n",
    "                    self.__setattr__(name, new_layer)\n",
    "                else:\n",
    "                    setattr(model, name, new_layer)\n",
    "            \n",
    "            else:\n",
    "                # Recursive call for child modules\n",
    "                self.replace_multihead_attention_recursion(heads_only, model=module)\n",
    "                \n",
    "                \n",
    "    def freeze_parameters_except_lora_and_bias(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            is_trainable = (\n",
    "                \"lora_\" in name\n",
    "                #(self.train_layer_norms and \"LayerNorm\" in name)\n",
    "            )\n",
    "\n",
    "            param.requires_grad = is_trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 48 (inside ./meta.pkl)\n"
     ]
    }
   ],
   "source": [
    "# Attempt to derive vocab_size from the dataset\n",
    "\n",
    "meta_path = os.path.join('./', 'meta.pkl')\n",
    "#vocab_size = None\n",
    "\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {vocab_size} (inside {meta_path})\")\n",
    "else:\n",
    "    print(\"Meta file not found. Please ensure the meta.pkl file is present in the data directory.\")\n",
    "\n",
    "# Encode and decode functions for character-level Tokenzation \n",
    "def encode(s):\n",
    "    return [meta['stoi'][c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    result=''\n",
    "    for i in l:\n",
    "        try:\n",
    "            result+=meta['itos'][i]\n",
    "        except:\n",
    "            result+='$'\n",
    "\n",
    "                       \n",
    "    return result\n",
    "        \n",
    "    return ''.join([meta['itos'][i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "        \"\"\"\n",
    "        Load pre-trained model based on the provided model name.\n",
    "        \"\"\"\n",
    "        model_path = os.path.join('./', f\"{model_name}.pth\")\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model file '{model_path}' not found.\")\n",
    "        \n",
    "        model = GPT()\n",
    "        print(\"Compiling the model...\\n\")\n",
    "        r = -1\n",
    "        if compile:\n",
    "            try:\n",
    "                model = torch.compile(model)  # requires PyTorch 2.0\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "            checkpoint = torch.load(model_path, map_location='cpu')\n",
    "            if 'lora_rank' in checkpoint.keys():\n",
    "                r = checkpoint['lora_rank']\n",
    "                print('lora randk detected')\n",
    "                state = checkpoint['state_dict']\n",
    "\n",
    "                if r > 0:\n",
    "                    model.activate_lora(r)\n",
    "                model.load_state_dict(state,strict=False)\n",
    "            else:\n",
    "                model.load_state_dict(checkpoint,strict=False)\n",
    "        else:\n",
    "            checkpoint = torch.load(model_path, map_location='cpu')\n",
    "            if 'lora_rank' in checkpoint.keys():\n",
    "                r = checkpoint['lora_rank']\n",
    "                state_dict = checkpoint['state_dict']\n",
    "\n",
    "                if r > 0:\n",
    "                    model.activate_lora(r)\n",
    "            else:\n",
    "                state_dict = checkpoint\n",
    "            \n",
    "            model.activate_lora(lora_r)\n",
    "            print('lora activated')\n",
    "            state_dict_keys = map(lambda x: x.replace(\"_orig_mod.\", \"\"), state_dict.keys())\n",
    "            state_dict = dict(zip(state_dict_keys, state_dict.values()))\n",
    "            model.load_state_dict(state_dict,strict=False)\n",
    "\n",
    "        m = model.to('cpu')\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the model...\n",
      "\n",
      "lora activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6345/2330487642.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GPT:\n\tsize mismatch for token_embedding_table.weight: copying a param with shape torch.Size([53, 372]) from checkpoint, the shape in current model is torch.Size([48, 372]).\n\tsize mismatch for lm_head.lora_b: copying a param with shape torch.Size([20, 53]) from checkpoint, the shape in current model is torch.Size([20, 48]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m num_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad)\n",
      "Cell \u001b[0;32mIn[25], line 44\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     state_dict_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_orig_mod.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m), state_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     43\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(state_dict_keys, state_dict\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m m \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GPT:\n\tsize mismatch for token_embedding_table.weight: copying a param with shape torch.Size([53, 372]) from checkpoint, the shape in current model is torch.Size([48, 372]).\n\tsize mismatch for lm_head.lora_b: copying a param with shape torch.Size([20, 53]) from checkpoint, the shape in current model is torch.Size([20, 48])."
     ]
    }
   ],
   "source": [
    "model=load_model()\n",
    "model=model.to('cpu')\n",
    "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {num_parameters} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.memmap('test.bin', dtype=np.uint16, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:11<00:00, 14.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def evaluate_example(example, model, max_new_tokens=2):\n",
    "\n",
    "    # Split example and determine maximum new tokens allowed\n",
    "    splited_example = example.split(\"# clone\")\n",
    "    # Encode prompt and prepare for evaluation\n",
    "    encoded_example = torch.tensor(encode(splited_example[0] + \"# clone\"), dtype=torch.long).unsqueeze(0).to('cpu')\n",
    "    prompt_text = splited_example[0] + \"# clone\"\n",
    "\n",
    "    result_example = splited_example[-1]\n",
    "    \n",
    "\n",
    "\n",
    "    response = decode(model.generate(encoded_example, max_new_tokens=max_new_tokens)[0].tolist())\n",
    "    splited_response = response.split(\"# clone\")\n",
    "    result_response = splited_response[-1]\n",
    "    #print(result_response)\n",
    "    \n",
    "    #generated_results = [float(match.group()) for match in re.finditer(r\"(?<=# )-?\\d+(\\.\\d+)?\", result_response.split('\\n\\n')[0].replace(\"\\n\", \"\"))]\n",
    "\n",
    "    return prompt_text, result_example, result_response\n",
    "\n",
    "\n",
    "\n",
    "# Write results to file\n",
    "def write_results_to_file(output_file, prompt, real_results, generated_results):\n",
    "    df = pd.DataFrame({\n",
    "        'Prompt': prompt,\n",
    "        'Real_Results': real_results,\n",
    "        'Generated_Results': generated_results\n",
    "    })\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_pair(real, generated_result):\n",
    "    # Determine the length of the shorter and longer strings\n",
    "    min_len = min(len(real), len(generated_result))\n",
    "    max_len = max(len(real), len(generated_result))\n",
    "\n",
    "    # Count the number of matching characters at the same index\n",
    "    match_count = sum(1 for i in range(min_len) if real[i] == generated_result[i])\n",
    "\n",
    "    # Calculate the ratio of matches to the length of the longer string\n",
    "    ratio = match_count / max_len\n",
    "    return ratio\n",
    "\n",
    "# Evaluation Loop\n",
    "\n",
    "# Split examples and initialize lists for results\n",
    "examples = decode(test_data).split(\"\\n\\n\\n\\n\")\n",
    "print(len(examples))\n",
    "\n",
    "examples = [example for example in examples if example]\n",
    "\n",
    "# Start evaluation process\n",
    "prompt = []\n",
    "real_results = []\n",
    "generated_results = []\n",
    "\n",
    "# Iterate through examples and evaluate the model on each one\n",
    "for example in tqdm(examples):\n",
    "    \n",
    "    prompt_text, real_result, result = evaluate_example(example, model)\n",
    "    prompt.append(prompt_text)\n",
    "    real_results.append(real_result)\n",
    "    generated_results.append(result)\n",
    "    \n",
    "       \n",
    "        \n",
    "    \n",
    "\n",
    "# Calculate and print accuracy\n",
    "score=0\n",
    "\n",
    "for real,generated in zip(real_results, generated_results):\n",
    "  score+=evaluate_pair(real,generated)\n",
    "accuracy = score / len(generated_results)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Store accuracy in a file\n",
    "with open(\"accuracy.txt\", 'w') as f:\n",
    "    f.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "# Store predictions in a CSV file\n",
    "    write_results_to_file(\"predictions.csv\", prompt, real_results, generated_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
