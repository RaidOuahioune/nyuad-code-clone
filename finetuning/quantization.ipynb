{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47b3d7e6",
   "metadata": {
    "_cell_guid": "aa556b17-8ea0-4788-aea8-8d6259526157",
    "_uuid": "a14f6813-426a-4666-9280-7ed88ebdb85e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-21T23:47:24.268575Z",
     "iopub.status.busy": "2024-08-21T23:47:24.268208Z",
     "iopub.status.idle": "2024-08-21T23:47:30.412761Z",
     "shell.execute_reply": "2024-08-21T23:47:30.411731Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 6.159801,
     "end_time": "2024-08-21T23:47:30.415208",
     "exception": false,
     "start_time": "2024-08-21T23:47:24.255407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7066f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu121\n",
      "12.1\n",
      "tensor([1.2567], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "\n",
    "print(torch.randn(1).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbc3d821",
   "metadata": {
    "_cell_guid": "063623b6-121d-4c18-a660-93d2f1be3305",
    "_uuid": "f10e66ef-f466-4cfc-8ddb-594df92adb45",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-21T23:47:30.439946Z",
     "iopub.status.busy": "2024-08-21T23:47:30.439378Z",
     "iopub.status.idle": "2024-08-21T23:47:30.445706Z",
     "shell.execute_reply": "2024-08-21T23:47:30.444831Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020845,
     "end_time": "2024-08-21T23:47:30.447696",
     "exception": false,
     "start_time": "2024-08-21T23:47:30.426851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to cuda.\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set the device to GPU if available, otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device set to {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5182d93",
   "metadata": {
    "_cell_guid": "4776333c-08cd-4127-bea7-d7ec8898df7b",
    "_uuid": "f61836e4-3f71-432d-8c50-9de1ff2e05e0",
    "papermill": {
     "duration": 0.01118,
     "end_time": "2024-08-21T23:47:30.470259",
     "exception": false,
     "start_time": "2024-08-21T23:47:30.459079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eed286a1",
   "metadata": {
    "_cell_guid": "fcc4b173-f5e5-4110-b14f-46a8fa6da9ae",
    "_uuid": "0aa1c1b8-a945-4baa-8d46-3a08056a9004",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-21T23:47:30.495216Z",
     "iopub.status.busy": "2024-08-21T23:47:30.494586Z",
     "iopub.status.idle": "2024-08-21T23:47:30.499991Z",
     "shell.execute_reply": "2024-08-21T23:47:30.499124Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020037,
     "end_time": "2024-08-21T23:47:30.501930",
     "exception": false,
     "start_time": "2024-08-21T23:47:30.481893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions to load and save data\n",
    "def save_data(data, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(data)\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "704df32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVEL=1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "185ed219",
   "metadata": {
    "_cell_guid": "9731ee3f-b4d1-4b6e-afb2-859c56bef6c6",
    "_uuid": "3da5ca68-e0d7-4aed-b89f-5f2a4ab910d9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-21T23:47:30.527568Z",
     "iopub.status.busy": "2024-08-21T23:47:30.526687Z",
     "iopub.status.idle": "2024-08-21T23:47:30.531045Z",
     "shell.execute_reply": "2024-08-21T23:47:30.530206Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.018853,
     "end_time": "2024-08-21T23:47:30.533093",
     "exception": false,
     "start_time": "2024-08-21T23:47:30.514240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Directory where the data is stored\n",
    "DATA_DIR = f'./../data/clones_level{LEVEL}.txt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9058c80d",
   "metadata": {
    "_cell_guid": "ddae0037-0f42-425d-a2e9-4238f4c608f2",
    "_uuid": "6d064118-585d-46a9-8f40-f9472fe879b4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-21T23:47:30.557742Z",
     "iopub.status.busy": "2024-08-21T23:47:30.557021Z",
     "iopub.status.idle": "2024-08-21T23:47:30.575883Z",
     "shell.execute_reply": "2024-08-21T23:47:30.574856Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.0335,
     "end_time": "2024-08-21T23:47:30.578022",
     "exception": false,
     "start_time": "2024-08-21T23:47:30.544522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 35 (inside ./../data/clones_level1.1.txt/meta.pkl)\n"
     ]
    }
   ],
   "source": [
    "# Attempt to derive vocab_size from the dataset\n",
    "\n",
    "meta_path = os.path.join(DATA_DIR, 'meta.pkl')\n",
    "#vocab_size = None\n",
    "\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {vocab_size} (inside {meta_path})\")\n",
    "else:\n",
    "    print(\"Meta file not found. Please ensure the meta.pkl file is present in the data directory.\")\n",
    "\n",
    "# Encode and decode functions for character-level Tokenzation \n",
    "def encode(s):\n",
    "    return [meta['stoi'][c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    result=''\n",
    "    for i in l:\n",
    "        try:\n",
    "            result+=meta['itos'][i]\n",
    "        except:\n",
    "            result+='$'\n",
    "\n",
    "                       \n",
    "    return result\n",
    "        \n",
    "    return ''.join([meta['itos'][i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96023d3b",
   "metadata": {
    "_cell_guid": "c53f3930-8d16-443d-a5ec-a6926f3f6cf4",
    "_uuid": "9cd8ff5a-2170-4c53-be17-02ac7d0cffd9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-21T23:47:30.602737Z",
     "iopub.status.busy": "2024-08-21T23:47:30.602092Z",
     "iopub.status.idle": "2024-08-21T23:47:30.613002Z",
     "shell.execute_reply": "2024-08-21T23:47:30.612106Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025414,
     "end_time": "2024-08-21T23:47:30.615066",
     "exception": false,
     "start_time": "2024-08-21T23:47:30.589652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data = np.memmap(os.path.join(DATA_DIR, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(DATA_DIR, 'val.bin'), dtype=np.uint16, mode='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c23094",
   "metadata": {
    "_cell_guid": "8574d987-cef6-47d1-b889-e8242a0bcd23",
    "_uuid": "f4fc1523-1d72-49db-a3bc-8d521f236993",
    "papermill": {
     "duration": 0.011105,
     "end_time": "2024-08-21T23:47:30.637608",
     "exception": false,
     "start_time": "2024-08-21T23:47:30.626503",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c2b8c79",
   "metadata": {
    "_cell_guid": "2d4305c5-c1c6-48b0-a048-953a98954854",
    "_uuid": "1fd63d8c-f842-444c-9dc8-cab3263ae6e4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-21T23:47:30.661850Z",
     "iopub.status.busy": "2024-08-21T23:47:30.661495Z",
     "iopub.status.idle": "2024-08-21T23:47:30.668603Z",
     "shell.execute_reply": "2024-08-21T23:47:30.667667Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021711,
     "end_time": "2024-08-21T23:47:30.670582",
     "exception": false,
     "start_time": "2024-08-21T23:47:30.648871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters for the GPT model\n",
    "block_size = 256  # Maximum context length\n",
    "n_embd = 372      # Embedding dimension\n",
    "n_head = 6        # Number of attention heads\n",
    "n_layer = 6       # Number of transformer blocks\n",
    "dropout = 0       # Dropout rate\n",
    "batch_size = 64   # Batch size for training\n",
    "max_iters = 100  # Maximum number of iterations\n",
    "learning_rate = 1e-3 # Initial Learning rate value\n",
    "miles = [int(max_iters * m) for m in [0.7, 0.8, 0.9]]  # Milestones for learning rate decay as fractions of max_iters\n",
    "eval_interval = 29_000 # Evaluation interval\n",
    "eval_iters = 60000 # Number of iterations for evaluation\n",
    "vocab_size = 53 # Vocabulary size\n",
    "\n",
    "# Model to be fine-tuned (Keep it empty for training from scratch)\n",
    "model_name = '10M_2024-07-21_08-16'\n",
    "model_path = \"./../models/10M_2024-07-21_08-16.pth\"\n",
    "\n",
    "#Lora params\n",
    "heads_only=False # fine tune only on attention head or all linear layers\n",
    "# LoRA Rank - Set it to 0 if you want to train from scratch or perform full fine-tuning\n",
    "lora_r = 10\n",
    "# Lora Alpha\n",
    "lora_alpha=8\n",
    "lora_dropout=0\n",
    "\n",
    "compile = False\n",
    "# enable quntization for the fine tuning (8 bit quantization)\n",
    "quantize=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8096f5a3",
   "metadata": {
    "_cell_guid": "17ff6e02-86d2-4f49-a384-be8c035377a7",
    "_uuid": "9c3a2af2-99a7-4657-bb8d-168a3e8dfcfb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-21T23:47:30.695450Z",
     "iopub.status.busy": "2024-08-21T23:47:30.695057Z",
     "iopub.status.idle": "2024-08-21T23:47:30.733433Z",
     "shell.execute_reply": "2024-08-21T23:47:30.732488Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.053625,
     "end_time": "2024-08-21T23:47:30.735609",
     "exception": false,
     "start_time": "2024-08-21T23:47:30.681984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm with an optional bias. PyTorch's LayerNorm doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        out = torch.nn.functional.scaled_dot_product_attention(\n",
    "            q, k, v, attn_mask=None, dropout_p=dropout if self.training else 0, is_causal=True\n",
    "        )\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Concatenate the outputs from each head\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple linear layer followed by a non-linearity.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd, bias=False),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class LinearLoRA(nn.Module):\n",
    "    def __init__(self, original_layer, rank=8):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.original_layer.weight.requires_grad = False\n",
    "        self.rank = rank\n",
    "        \n",
    "        self.lora_a = nn.Parameter(torch.randn((original_layer.in_features, rank)))\n",
    "        self.lora_b = nn.Parameter(torch.randn((rank, original_layer.out_features)))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.lora_a, a=np.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_b)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lora_output = x @ self.lora_a @ self.lora_b\n",
    "        return self.original_layer(x) + lora_output\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by feedforward.\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd, bias=False)\n",
    "        self.ln2 = nn.LayerNorm(n_embd, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"GPT language model.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd, bias=False) \n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Token and position embeddings\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n",
    "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
    "        x = self.blocks(x) # (B, T, n_embd)\n",
    "        x = self.ln_f(x) # (B, T, n_embd)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        # Compute loss if targets are provided\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"Generate new tokens given an initial context `idx`.\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:] # Crop to the last block_size tokens\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # Focus on the last time step\n",
    "            probs = F.softmax(logits, dim=-1) # Convert to probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # Sample from the distribution\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # Append sampled index to the sequence\n",
    "        return idx\n",
    "    \n",
    "    def activate_lora(self, r=8, heads_only=False, freeze_others=True):\n",
    "        self.lora_rank = r\n",
    "        self.replace_multihead_attention_recursion(heads_only)\n",
    "        if freeze_others:\n",
    "            self.freeze_parameters_except_lora_and_bias()\n",
    "    \n",
    "    def replace_multihead_attention_recursion(self, heads_only=False, model=None):\n",
    "        children = self.named_children() if model is None else model.named_children()\n",
    "        for name, module in children:\n",
    "            if heads_only and name in {\"query\", \"key\", \"value\"}:\n",
    "                # Replace with Lora SelfAttention\n",
    "                new_layer = LinearLoRA(module, rank=self.lora_rank)\n",
    "\n",
    "                if model == None:\n",
    "                    self.__setattr__(name, new_layer)\n",
    "                else:\n",
    "                    setattr(model, name, new_layer)\n",
    "            \n",
    "            elif isinstance(module, nn.Linear) and not heads_only:\n",
    "                new_layer = LinearLoRA(module, rank=self.lora_rank)\n",
    "                \n",
    "                if model == None:\n",
    "                    self.__setattr__(name, new_layer)\n",
    "                else:\n",
    "                    setattr(model, name, new_layer)\n",
    "            \n",
    "            else:\n",
    "                # Recursive call for child modules\n",
    "                self.replace_multihead_attention_recursion(heads_only, model=module)\n",
    "                \n",
    "    def get_all_linear_layer_names(self,layers=[],model=None):\n",
    "       \n",
    "        children = self.named_children() if model is None else model.named_children()\n",
    "        for name, module in children:\n",
    "\n",
    "            if isinstance(module, nn.Linear) :\n",
    "                layers.append(name)\n",
    "            else:\n",
    "                self.get_all_linear_layer_names(layers, model=module)\n",
    "                \n",
    "        return layers\n",
    "    \n",
    "    \n",
    "    def get_all_linear_param_names(self):\n",
    "        linear_param_names = []\n",
    "\n",
    "        # Iterate over all named parameters in the model\n",
    "        for name, param in self.named_parameters():\n",
    "            # Get the module name by splitting at the first period\n",
    "            module_name = name.split('.')[0]\n",
    "\n",
    "            # Check if the module is of type nn.Linear\n",
    "            module = dict(self.named_modules())[module_name]\n",
    "            if isinstance(module, nn.Linear):\n",
    "                linear_param_names.append(name)\n",
    "\n",
    "        return linear_param_names\n",
    "\n",
    "            \n",
    "    def freeze_parameters_except_lora_and_bias(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            is_trainable = (\n",
    "                \"lora_\" in name\n",
    "                #(self.train_layer_norms and \"LayerNorm\" in name)\n",
    "            )\n",
    "\n",
    "            param.requires_grad = is_trainable\n",
    "\n",
    "    def unfreeze_all_parameters(self):\n",
    "        for param in self.parameters():\n",
    "\n",
    "            param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9b4961b",
   "metadata": {
    "_cell_guid": "a716f789-f605-42d0-9494-d8927ed09a6f",
    "_uuid": "be441d8d-c18b-4694-b2ff-607aac4b11e6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-21T23:47:30.760529Z",
     "iopub.status.busy": "2024-08-21T23:47:30.759700Z",
     "iopub.status.idle": "2024-08-21T23:47:30.769985Z",
     "shell.execute_reply": "2024-08-21T23:47:30.769217Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025071,
     "end_time": "2024-08-21T23:47:30.772078",
     "exception": false,
     "start_time": "2024-08-21T23:47:30.747007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get random batch of data\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Estimate loss on train and val splits\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters) \n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "# Helper function to make large numbers of parameters human-readable\n",
    "def human_readable(num):\n",
    "    magnitude = 0\n",
    "    while abs(num) >= 1000:\n",
    "        magnitude += 1\n",
    "        num /= 1000.0\n",
    "    return '%.0f%s' % (num, ['', 'K', 'M', 'G', 'T', 'P'][magnitude])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "555bbd4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T23:47:30.797543Z",
     "iopub.status.busy": "2024-08-21T23:47:30.796778Z",
     "iopub.status.idle": "2024-08-21T23:47:30.807300Z",
     "shell.execute_reply": "2024-08-21T23:47:30.806325Z"
    },
    "papermill": {
     "duration": 0.025358,
     "end_time": "2024-08-21T23:47:30.809381",
     "exception": false,
     "start_time": "2024-08-21T23:47:30.784023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from accelerate.utils import BnbQuantizationConfig,load_and_quantize_model\n",
    "def load_model( ):\n",
    "\n",
    "    \n",
    "    model = GPT()  # Replace with your model initialization\n",
    "    print(\"Loading the model...\\n\")\n",
    "    if quantize:\n",
    "        print(\"Quantizing the model\")\n",
    "        quantization_config= BnbQuantizationConfig(\n",
    "            load_in_8bit=True,  # Load model with 8-bit precision \n",
    "            bnb_4bit_compute_dtype=torch.float16,  # Use bfloat16 for computations to balance precision and efficiency.\n",
    "            bnb_4bit_use_double_quant=True,  # Apply double quantization to minimize quantization errors.\n",
    "            bnb_4bit_quant_type=\"nf4\"  # Use \"Normal Float 4\" (nf4) quantization for better numerical stability.\n",
    "        )\n",
    "        model = load_and_quantize_model(model,\n",
    "                                                  weights_location=model_path,\n",
    "                                                  bnb_quantization_config=quantization_config,\n",
    "                                                  device_map = device)\n",
    "    \n",
    "    else:\n",
    "        state_dict=torch.load(model_path, map_location=device)\n",
    "        state_dict_keys = map(lambda x: x.replace(\"_orig_mod.\", \"\"), state_dict.keys())\n",
    "        state_dict = dict(zip(state_dict_keys, state_dict.values()))\n",
    "        model.load_state_dict(state_dict)\n",
    "        print(\"loading the model stat\")\n",
    "    if lora_r>0:\n",
    "        \"\"\"  \n",
    "        if heads_only:\n",
    "            print(\"Applying Lora on heads only ...\")\n",
    "            target_modules=['query', 'key', 'value']\n",
    "        else:\n",
    "            print(\"Applying Lora on all linear layers\")\n",
    "            target_modules=['query', 'key', 'value','proj']\n",
    "        \n",
    "        peft_config = peft.LoraConfig(\n",
    "                r=lora_r, # rank dimension of the LoRA injected matrices\n",
    "                lora_alpha=lora_alpha, \n",
    "                target_modules=target_modules, # the modules to add lora params to\n",
    "                lora_dropout=lora_dropout, # dropout probability for layers\n",
    "                bias=\"lora_only\", # none, all, or lora_only\n",
    "            )\n",
    "        \"\"\"\n",
    "        model.activate_lora(lora_r)\n",
    "       \n",
    "        print(\"lora activation\")\n",
    "        \n",
    "    model = model.to(device)\n",
    "\n",
    "    \n",
    "    # Apply PEFT if a configuration is provided\n",
    "    \"\"\"\n",
    "    if peft_config:\n",
    "        model =  peft.get_peft_model(model, peft_config)\n",
    "        print(\"Applied PEFT (e.g., LoRA) to the model.\")\n",
    "    if not peft_config and not quantize:\n",
    "        print(\"Loading the model with no lora or quantization\")\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint,strict=False)\n",
    "    \"\"\"\n",
    "        \n",
    "\n",
    "        \n",
    "    if compile:\n",
    "        model= torch.compile(model) \n",
    "        model= model.to(device)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "033e1ec1",
   "metadata": {
    "_cell_guid": "21de39d0-d298-45ce-a590-c6be400f31e8",
    "_uuid": "db1edcb0-7dae-40b8-99f0-3a524bd1311e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-21T23:47:30.834388Z",
     "iopub.status.busy": "2024-08-21T23:47:30.834006Z",
     "iopub.status.idle": "2024-08-21T23:47:31.703093Z",
     "shell.execute_reply": "2024-08-21T23:47:31.702221Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.884163,
     "end_time": "2024-08-21T23:47:31.705351",
     "exception": false,
     "start_time": "2024-08-21T23:47:30.821188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model...\n",
      "\n",
      "loading the model stat\n",
      "lora activation\n",
      "The model has 741K trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4741/3813130049.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict=torch.load(model_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "model=load_model()\n",
    "model=model.to(device)\n",
    "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "num_parameters_hr = human_readable(num_parameters)\n",
    "print(f'The model has {num_parameters_hr} trainable parameters')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156af52e",
   "metadata": {
    "_cell_guid": "ac1fe251-e0c8-4079-9da4-68aff59262f4",
    "_uuid": "8cdf45cc-0d3a-43a9-b10d-5381799a21f2",
    "papermill": {
     "duration": 0.011301,
     "end_time": "2024-08-21T23:47:31.728271",
     "exception": false,
     "start_time": "2024-08-21T23:47:31.716970",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4362c02",
   "metadata": {
    "_cell_guid": "e725706a-19a1-4e82-91b1-514dd0488f33",
    "_uuid": "45093d41-9498-45e4-b93b-95b0b239c0af",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-21T23:47:31.753151Z",
     "iopub.status.busy": "2024-08-21T23:47:31.752784Z",
     "iopub.status.idle": "2024-08-21T23:47:32.382389Z",
     "shell.execute_reply": "2024-08-21T23:47:32.381498Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.64509,
     "end_time": "2024-08-21T23:47:32.385006",
     "exception": false,
     "start_time": "2024-08-21T23:47:31.739916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "if not quantize:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Initialize learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=miles, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3819af6",
   "metadata": {
    "_cell_guid": "76b8e469-893d-4151-a175-99b54dbabe60",
    "_uuid": "534a6c6a-e6b8-4632-8078-86aab93500de",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-21T23:47:32.410276Z",
     "iopub.status.busy": "2024-08-21T23:47:32.409695Z",
     "iopub.status.idle": "2024-08-22T03:59:10.137579Z",
     "shell.execute_reply": "2024-08-22T03:59:10.136625Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 15097.757184,
     "end_time": "2024-08-22T03:59:10.153994",
     "exception": false,
     "start_time": "2024-08-21T23:47:32.396810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Training time: 0.5772793134053548  min\n",
      "Model saved to ./../models/level1.1/741K_2024-08-27_13-45.pth\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get current date and hour to get track of experiments\n",
    "now = datetime.datetime.now()\n",
    "date_hour = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "# Train\n",
    "# Start training timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # evaluate the model on the train and val splits and log the losses\n",
    "    #if iter % eval_interval == 0:\n",
    "    #    losses = estimate_loss()\n",
    "    #    print(f'iter {iter:5d} | train loss {losses[\"train\"]:.4f} | val loss {losses[\"val\"]:.4f}')\n",
    "        \n",
    "    # train the model for one iteration\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    #loss.requires_grad = True\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    print(iter)\n",
    "\n",
    "# End training timer\n",
    "end_time = time.time()\n",
    "print(f'Training time: {(end_time - start_time) / 60}  min')\n",
    "\n",
    "# crucial in case u want to load the model again\n",
    "model.unfreeze_all_parameters()\n",
    "\n",
    "# Save the trained model\n",
    "model_path = f\"./../models/level{LEVEL}/{num_parameters_hr}_{date_hour}.pth\"\n",
    "checkpoint = {\n",
    "    'lora_rank': model.lora_rank if(hasattr(model, \"lora_rank\")) else -1,\n",
    "    'state_dict': model.state_dict()\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, model_path)\n",
    "print(f\"Model saved to {model_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273691b8",
   "metadata": {
    "_cell_guid": "e831564c-6b76-489b-98b0-69cad098fdd6",
    "_uuid": "facd8250-1fd4-4486-a9a6-f099df266caf",
    "papermill": {
     "duration": 0.011196,
     "end_time": "2024-08-22T03:59:10.176467",
     "exception": false,
     "start_time": "2024-08-22T03:59:10.165271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b16bd794",
   "metadata": {
    "_cell_guid": "d8071f1a-961b-4410-ae36-ba54b5b525d0",
    "_uuid": "f4e10d4c-a4c8-4e6b-891e-f3d14947adfb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-22T03:59:10.201888Z",
     "iopub.status.busy": "2024-08-22T03:59:10.201553Z",
     "iopub.status.idle": "2024-08-22T03:59:10.222752Z",
     "shell.execute_reply": "2024-08-22T03:59:10.222019Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.036597,
     "end_time": "2024-08-22T03:59:10.224700",
     "exception": false,
     "start_time": "2024-08-22T03:59:10.188103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = np.memmap(os.path.join(DATA_DIR, 'test.bin'), dtype=np.uint16, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "55001c96",
   "metadata": {
    "_cell_guid": "f3d6ae4b-e069-43bd-be3f-9e46f19146d3",
    "_uuid": "2e9f95ba-ca83-48bc-bb18-8910efc37422",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-22T03:59:10.249188Z",
     "iopub.status.busy": "2024-08-22T03:59:10.248546Z",
     "iopub.status.idle": "2024-08-22T03:59:11.562139Z",
     "shell.execute_reply": "2024-08-22T03:59:11.561077Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.328405,
     "end_time": "2024-08-22T03:59:11.564522",
     "exception": false,
     "start_time": "2024-08-22T03:59:10.236117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:34<00:00, 28.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "Accuracy: 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def evaluate_example(example, model, max_new_tokens=2):\n",
    "\n",
    "    # Split example and determine maximum new tokens allowed\n",
    "    splited_example = example.split(\"# clone\")\n",
    "    # Encode prompt and prepare for evaluation\n",
    "    encoded_example = torch.tensor(encode(splited_example[0] + \"# clone\"), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    prompt_text = splited_example[0] + \"# clone\"\n",
    "\n",
    "    result_example = splited_example[-1]\n",
    "\n",
    "\n",
    "    response = decode(model.generate(encoded_example, max_new_tokens=max_new_tokens)[0].tolist())\n",
    "    splited_response = response.split(\"# clone\")\n",
    "    result_response = splited_response[-1]\n",
    "    return prompt_text, result_example, result_response\n",
    "\n",
    "\n",
    "\n",
    "# Write results to file\n",
    "def write_results_to_file(output_file, prompt, real_results, generated_results):\n",
    "    df = pd.DataFrame({\n",
    "        'Prompt': prompt,\n",
    "        'Real_Results': real_results,\n",
    "        'Generated_Results': generated_results\n",
    "    })\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation Loop\n",
    "\n",
    "# Split examples and initialize lists for results\n",
    "examples = decode(test_data).split(\"\\n\\n\\n\\n\")\n",
    "print(len(examples))\n",
    "\n",
    "examples = [example for example in examples if example]\n",
    "\n",
    "# Start evaluation process\n",
    "prompt = []\n",
    "real_results = []\n",
    "generated_results = []\n",
    "\n",
    "# Iterate through examples and evaluate the model on each one\n",
    "for example in tqdm(examples):\n",
    "    \n",
    "        prompt_text, real_result, result = evaluate_example(example, model)\n",
    "        prompt.append(prompt_text)\n",
    "        real_results.append(real_result)\n",
    "        generated_results.append(result)\n",
    "    \n",
    "       \n",
    "        \n",
    "    \n",
    "\n",
    "# Calculate and print accuracy\n",
    "score=0\n",
    "\n",
    "for real,generated in zip(real_results, generated_results):\n",
    "    print(len(real))\n",
    "    print(len(generated))\n",
    "    if real==generated:\n",
    "        score+=1\n",
    "accuracy = score / len(generated_results)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Store accuracy in a file\n",
    "with open(\"accuracy.txt\", 'w') as f:\n",
    "    f.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "# Store predictions in a CSV file\n",
    "    write_results_to_file(\"predictions.csv\", prompt, real_results, generated_results)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5419152,
     "sourceId": 9104825,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5498826,
     "sourceId": 9110656,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5527817,
     "sourceId": 9175514,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5575850,
     "sourceId": 9220383,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 99718,
     "modelInstanceId": 74994,
     "sourceId": 89395,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15143.230293,
   "end_time": "2024-08-22T03:59:13.029368",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-21T23:46:49.799075",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
